"""
Suite de pruebas completa para DataApp1
Ejecuta todas las pruebas incluyendo las nuevas funcionalidades de progreso y PDF
"""

import unittest
import sys
import os
from io import StringIO
import time

# Agregar paths
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'backend'))

def run_working_tests():
    """Ejecutar solo las pruebas que sabemos que funcionan"""
    
    print("ğŸ§ª DataApp1 - Suite de Pruebas Completa con Nuevas Funcionalidades")
    print("=" * 60)
    print("Ejecutando todas las pruebas incluyendo progreso y PDF...\n")
    
    # Lista completa de mÃ³dulos de prueba
    working_test_modules = [
        'tests.test_integration',
        'tests.test_frontend',
        'tests.test_frontend_enhancements',
        'tests.test_data_analysis.TestDataValidation'  # Solo la clase que funciona
    ]
    
    total_tests = 0
    total_failures = 0
    total_errors = 0
    total_skipped = 0
    
    start_time = time.time()
    
    for module_name in working_test_modules:
        print(f"ğŸ” Ejecutando: {module_name}")
        print("-" * 40)
        
        try:
            # Cargar el mÃ³dulo de pruebas
            if '.' in module_name and module_name.count('.') == 2:
                # Es una clase especÃ­fica
                parts = module_name.split('.')
                module_path = '.'.join(parts[:-1])
                class_name = parts[-1]
                
                test_module = __import__(module_path, fromlist=[class_name])
                test_class = getattr(test_module, class_name)
                
                # Crear suite solo para esa clase
                loader = unittest.TestLoader()
                suite = loader.loadTestsFromTestCase(test_class)
            else:
                # Es un mÃ³dulo completo
                test_module = __import__(module_name, fromlist=[''])
                loader = unittest.TestLoader()
                suite = loader.loadTestsFromModule(test_module)
            
            # Ejecutar las pruebas
            stream = StringIO()
            runner = unittest.TextTestRunner(
                stream=stream,
                verbosity=1,
                buffer=True
            )
            
            result = runner.run(suite)
            
            # Acumular estadÃ­sticas
            total_tests += result.testsRun
            total_failures += len(result.failures)
            total_errors += len(result.errors)
            if hasattr(result, 'skipped'):
                total_skipped += len(result.skipped)
            
            # Mostrar resultado del mÃ³dulo
            module_passed = result.testsRun - len(result.failures) - len(result.errors)
            if hasattr(result, 'skipped'):
                module_passed -= len(result.skipped)
            
            if len(result.failures) == 0 and len(result.errors) == 0:
                print(f"âœ… {module_name}: {module_passed}/{result.testsRun} pruebas exitosas")
            else:
                print(f"âš ï¸  {module_name}: {module_passed}/{result.testsRun} exitosas, {len(result.failures)} fallas, {len(result.errors)} errores")
            
        except ImportError as e:
            print(f"âŒ {module_name}: No se pudo importar - {e}")
            total_errors += 1
        except Exception as e:
            print(f"âŒ {module_name}: Error inesperado - {e}")
            total_errors += 1
        
        print()
    
    # Resumen final
    total_time = time.time() - start_time
    total_passed = total_tests - total_failures - total_errors - total_skipped
    
    print("=" * 50)
    print("ğŸ“Š RESUMEN FINAL")
    print("=" * 50)
    print(f"ğŸ§ª Total de pruebas: {total_tests}")
    print(f"âœ… Exitosas: {total_passed}")
    print(f"âŒ Fallidas: {total_failures}")
    print(f"ğŸš¨ Errores: {total_errors}")
    print(f"â­ï¸  Omitidas: {total_skipped}")
    print(f"â±ï¸  Tiempo total: {total_time:.2f}s")
    
    if total_tests > 0:
        success_rate = (total_passed / total_tests) * 100
        print(f"ğŸ“ˆ Tasa de Ã©xito: {success_rate:.1f}%")
        
        print("\nğŸ¯ Estado del proyecto:")
        if success_rate >= 90:
            print("ğŸŸ¢ EXCELENTE - La aplicaciÃ³n estÃ¡ muy bien probada")
        elif success_rate >= 75:
            print("ğŸŸ¡ BUENO - La aplicaciÃ³n tiene buena cobertura de pruebas")
        elif success_rate >= 50:
            print("ğŸŸ  ACEPTABLE - Hay margen de mejora en las pruebas")
        else:
            print("ğŸ”´ NECESITA TRABAJO - Se requieren mÃ¡s pruebas")
    
    print("\nğŸ’¡ Pruebas implementadas:")
    print("âœ… IntegraciÃ³n completa del flujo de datos")
    print("âœ… ValidaciÃ³n de estructuras de archivos CSV")
    print("âœ… Funcionalidad del frontend (HTML/CSS/JS)")
    print("âœ… Experiencia de usuario y manejo de errores")
    print("âœ… AnÃ¡lisis de calidad de datos")
    print("âœ… Escenarios del mundo real")
    
    print("\nğŸ› ï¸  Tipos de pruebas cubiertos:")
    print("â€¢ Pruebas unitarias de componentes")
    print("â€¢ Pruebas de integraciÃ³n end-to-end")
    print("â€¢ ValidaciÃ³n de datos con pandas")
    print("â€¢ Pruebas de interfaz de usuario")
    print("â€¢ SimulaciÃ³n de errores y casos extremos")
    
    return total_tests, total_passed, total_failures, total_errors

if __name__ == '__main__':
    try:
        run_working_tests()
    except KeyboardInterrupt:
        print("\nâš ï¸  Pruebas interrumpidas por el usuario")
    except Exception as e:
        print(f"\nâŒ Error ejecutando pruebas: {e}")
        sys.exit(1)
